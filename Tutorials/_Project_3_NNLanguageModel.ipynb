{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from miniml.ann import *\n",
    "from miniml.ann.common import *\n",
    "from miniml.ann.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2        # number of words in each sentence\n",
    "d = 2        # embedding size\n",
    "n_hidden = 2 # hidden size\n",
    "\n",
    "sentences = [\"i like games\", \"i love programming\", \"i hate raining\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)  # size of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform to training format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "target_batch = []\n",
    "\n",
    "for sen in sentences:\n",
    "    word = sen.split()\n",
    "    input_ = [word_dict[n] for n in word[:-1]]\n",
    "    target = word_dict[word[-1]]\n",
    "\n",
    "    input_batch.append(input_)\n",
    "    target_batch.append(target)\n",
    "\n",
    "input_batch = np.array(input_batch)\n",
    "target_batch = np.array(target_batch)\n",
    "\n",
    "# one hot for target, because here we use MSE loss\n",
    "b = np.zeros((target_batch.size, n_class))\n",
    "b[np.arange(target_batch.size), target_batch] = 1\n",
    "target_batch = np.copy(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize weights, biases, and embedding table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(c*d, n_hidden)\n",
    "b1 = np.random.randn(n_hidden)\n",
    "\n",
    "W2 = np.random.randn(n_hidden, n_class)\n",
    "W3 = np.random.randn(c*d, n_class)\n",
    "b23 = np.random.randn(n_class)\n",
    "\n",
    "embedding = np.random.randn(n_class, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Neural Network Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_node, y_node = Placeholder(), Placeholder()\n",
    "\n",
    "table = Placeholder()\n",
    "\n",
    "W1_node = Placeholder()\n",
    "b1_node = Placeholder()\n",
    "\n",
    "W2_node = Placeholder()\n",
    "W3_node = Placeholder()\n",
    "b23_node = Placeholder()\n",
    "\n",
    "em_node = Embedding(x_node, table)\n",
    "l1 = Linear(em_node, W1_node, b1_node)\n",
    "a1 = Tanh(l1)\n",
    "l2 = Linear(a1, W2_node, None)\n",
    "l3 = Linear(em_node, W3_node, None)\n",
    "add = Add(l2, l3, b23_node)\n",
    "mse = MSE(y_node, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed initial values, use topology to create ordered graph for forward and backward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    x_node: input_batch,\n",
    "    y_node: target_batch,\n",
    "    \n",
    "    table: embedding,\n",
    "    \n",
    "    W1_node: W1,\n",
    "    b1_node: b1,\n",
    "\n",
    "    W2_node: W2,\n",
    "    W3_node: W3,\n",
    "    b23_node: b23\n",
    "}\n",
    "\n",
    "graph = feed_dict_2_graph(feed_dict)    # network graph\n",
    "sorted_graph = topology(graph)          # sorted graph\n",
    "trainables = [table, W1_node, b1_node, W2_node, W3_node, b23_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 0.1286\n",
      "Epoch: 1000, Loss: 0.0583\n",
      "Epoch: 1500, Loss: 0.0518\n",
      "Epoch: 2000, Loss: 0.0485\n",
      "Epoch: 2500, Loss: 0.0468\n",
      "Epoch: 3000, Loss: 0.0449\n",
      "Epoch: 3500, Loss: 0.0371\n",
      "Epoch: 4000, Loss: 0.0177\n",
      "Epoch: 4500, Loss: 0.0029\n",
      "Epoch: 5000, Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'Adam'\n",
    "lr=1e-3\n",
    "epochs=5000\n",
    "batch_size = 3\n",
    "\n",
    "steps_per_epoch = len(input_batch) // batch_size\n",
    "losses = []\n",
    "\n",
    "# only used for Adam\n",
    "it = 0\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        it += 1\n",
    "        # Step 4.1: sample a batch of examples and Reset value\n",
    "        x_node.value = input_batch[j*batch_size:(j+1)*batch_size]\n",
    "        y_node.value = target_batch[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "        # Step 4.2: forward\n",
    "        for n in sorted_graph:\n",
    "            n.forward()\n",
    "\n",
    "        # Step 4.3: backward\n",
    "        for n in sorted_graph[::-1]:\n",
    "            n.backward()\n",
    "\n",
    "        # Step 4.4: optimization\n",
    "        for t in trainables:\n",
    "            t.optimize(optimizer=optimizer, lr=lr, it=it)\n",
    "\n",
    "        # Step 5: update current loss\n",
    "        loss += sorted_graph[-1].value\n",
    "\n",
    "    if i % 500 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(i, loss/steps_per_epoch))\n",
    "        losses.append(loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\tpredicts\n",
      "\n",
      "i like -> games\n",
      "i love -> programming\n",
      "i hate -> raining\n"
     ]
    }
   ],
   "source": [
    "x_node.value = input_batch\n",
    "for n in sorted_graph[:-1]:\n",
    "    n.forward()\n",
    "preds = [number_dict[idx] for idx in np.argmax(sorted_graph[-2].value, axis=1)]\n",
    "\n",
    "print(f\"inputs\\tpredicts\\n\")\n",
    "for i, sen in enumerate(sentences):\n",
    "    print(f\"{' '.join(sen.split()[:-1])} -> {preds[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
